{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import xml.etree.cElementTree as et\n",
    "import re\n",
    "\n",
    "from collections import *\n",
    "import time\n",
    "\n",
    "import Stemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_dict = defaultdict(int)\n",
    "for word in stop_words:\n",
    "    stop_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_xml = \"./WikiDump_1/WikiDump_1.xml-p1p30303\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_external_links(body):\n",
    "    external_links = []\n",
    "    lines = body.split(\"==\")[-1]\n",
    "    lines = lines.split(\"\\n\")\n",
    " \n",
    "    for line in lines:\n",
    "        if re.match(r\"\\*(.*)\", line):\n",
    "            external_links.append(line)\n",
    " \n",
    "    return external_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki(body):\n",
    "    \n",
    "    body = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub('<!--.*?-->',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub('<math([> ].*?)(</math>|/>)',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub(r'\\[\\[([fF]ile:|[iI]mage)[^]]*(\\]\\])',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub(r'{{v?cite(.*?)}}',' ',body,flags = re.DOTALL)\n",
    "\n",
    "    References = re.findall(\"<ref>(.*?)</ref>\", body)\n",
    "    Infobox = re.findall(r\"\\{\\{Infobox (.*?)\\}\\}\", body, flags = re.DOTALL)\n",
    "    Category = re.findall(r\"\\[\\[Category:(.*?)\\]\\]\", body)\n",
    "    External = get_external_links(body)\n",
    "\n",
    "    body = re.sub('<.*?>',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub('{{([^}{]*)}}','',body,flags = re.DOTALL)\n",
    "    body = re.sub('{{([^}]*)}}','',body,flags = re.DOTALL)\n",
    "\n",
    "    return body.lower(), Infobox, Category, References, External"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = nltk.stem.SnowballStemmer('english')\n",
    "stemmer = Stemmer.Stemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_position = []\n",
    "title_list = []\n",
    "\n",
    "title_index = defaultdict(list)\n",
    "body_index = defaultdict(list)\n",
    "category_index = defaultdict(list)\n",
    "infobox_index = defaultdict(list)\n",
    "reference_index = defaultdict(list)\n",
    "external_index = defaultdict(list)\n",
    "\n",
    "stem_dict = defaultdict(int)\n",
    "token_count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170.53172560000166\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "PageCount = 0\n",
    "title_num = 0\n",
    "\n",
    "for idx, (event, elem) in enumerate(et.iterparse(wiki_xml, events=('start', 'end'))):\n",
    "\n",
    "    tag = elem.tag.split('}')[-1]\n",
    "    \n",
    "    if event == 'start':\n",
    "        if tag == 'page':\n",
    "            title_dict = defaultdict(int)\n",
    "            infobox_dict = defaultdict(int)\n",
    "            category_dict = defaultdict(int)\n",
    "            reference_dict = defaultdict(int)\n",
    "            external_dict = defaultdict(int)\n",
    "            id = -1\n",
    "            redirect = ''\n",
    "            inrevision = False\n",
    "            ns = 0\n",
    "            body_dict = defaultdict(int)\n",
    "        elif tag == 'revision':\n",
    "            # Do not pick up on revision id's\n",
    "            inrevision = True\n",
    "    \n",
    "    else:\n",
    "        if tag == 'title':\n",
    "            title = elem.text\n",
    "            if(title):\n",
    "                title_list.append(title)\n",
    "                title_num += 1\n",
    "        elif tag == 'id' and not inrevision:\n",
    "            id = int(elem.text)\n",
    "        elif tag == 'redirect':\n",
    "            redirect = elem.attrib['title']\n",
    "        elif tag == 'ns':\n",
    "            ns = int(elem.text)\n",
    "        elif tag == 'text':\n",
    "            body = elem.text\n",
    "        elif tag == 'page':\n",
    "            PageCount += 1\n",
    "            \n",
    "            body,Infobox,Category,Reference,External = clean_wiki(body)\n",
    "            ############# Acquiring word counts per document #############\n",
    "            \n",
    "            #### Title ####\n",
    "            title_words = re.split(\"[^a-zA-Z0-9]\",title.lower())\n",
    "            for word in title_words:\n",
    "                if(word):\n",
    "                    token_count[word] = 1\n",
    "                    word = stemmer.stemWord(word)\n",
    "                    if(len(word) > 2):\n",
    "                        title_dict[word] += 1\n",
    "                        \n",
    "            #### Infobox ####\n",
    "            for info in Infobox:\n",
    "                info_words = re.split(\"[^a-zA-Z0-9]\",info.lower())\n",
    "                for word in info_words:\n",
    "                    token_count[word] = 1\n",
    "                    if(word and not stop_dict[word]):\n",
    "                        if(not stem_dict[word]):\n",
    "                            stem_dict[word] = stemmer.stemWord(word)\n",
    "                        word = stem_dict[word]\n",
    "                        if(len(word) > 2):\n",
    "                            infobox_dict[word] += 1  \n",
    "                        \n",
    "            #### Category ####\n",
    "            for cate in Category:\n",
    "                cate_words = re.split(\"[^a-zA-Z0-9]\",cate.lower())\n",
    "                for word in cate_words:\n",
    "                    token_count[word] = 1\n",
    "                    if(word and not stop_dict[word]):\n",
    "                        if(not stem_dict[word]):\n",
    "                            stem_dict[word] = stemmer.stemWord(word)\n",
    "                        word = stem_dict[word]\n",
    "                        if(len(word) > 2):\n",
    "                            category_dict[word] += 1\n",
    "            \n",
    "            #### Reference ####\n",
    "            for ref in Reference:\n",
    "                ref_words = re.split(\"[^a-zA-Z0-9]\",ref.lower())\n",
    "                for word in ref_words:\n",
    "                    token_count[word] = 1\n",
    "                    if(word and not stop_dict[word]):\n",
    "                        word = stemmer.stemWord(word)\n",
    "                        if(len(word) > 2):\n",
    "                            reference_dict[word] += 1\n",
    "            \n",
    "            #### External ####\n",
    "            for ext in External:\n",
    "                ext_words = re.split(\"[^a-zA-Z0-9]\",ext.lower())\n",
    "                for word in ext_words:\n",
    "                    token_count[word] = 1\n",
    "                    if(word and not stop_dict[word]):\n",
    "                        if(not stem_dict[word]):\n",
    "                            stem_dict[word] = stemmer.stemWord(word)\n",
    "                        word = stem_dict[word]\n",
    "                        if(len(word) > 2):\n",
    "                            external_dict[word] += 1\n",
    "            \n",
    "            \n",
    "            #### Body ####\n",
    "            body_words = re.split(\"[^a-zA-Z0-9]\",body)\n",
    "            for word in body_words:\n",
    "                token_count[word] = 1\n",
    "                if(word and not stop_dict[word]):\n",
    "                    if(not stem_dict[word]):\n",
    "                        stem_dict[word] = stemmer.stemWord(word)\n",
    "                    word = stem_dict[word]\n",
    "                    if(len(word) > 2):\n",
    "                        body_dict[word] += 1\n",
    "            \n",
    "            ################ Index Creation ################\n",
    "            \n",
    "            for word in body_dict :     body_index[word].append(':'.join((str(PageCount),str(body_dict[word]))))\n",
    "\n",
    "            for word in title_dict :    title_index[word].append(':'.join((str(PageCount),str(title_dict[word]))))\n",
    "\n",
    "            for word in category_dict : category_index[word].append(':'.join((str(PageCount),str(category_dict[word]))))\n",
    "\n",
    "            for word in infobox_dict :  infobox_index[word].append(':'.join((str(PageCount),str(infobox_dict[word]))))\n",
    "            \n",
    "            for word in reference_dict :  reference_index[word].append(':'.join((str(PageCount),str(reference_dict[word]))))\n",
    "\n",
    "            for word in external_dict :  external_index[word].append(':'.join((str(PageCount),str(external_dict[word]))))\n",
    "\n",
    "            \n",
    "        elem.clear()\n",
    "#     if(PageCount == 30):\n",
    "#         break\n",
    "\n",
    "        \n",
    "end = time.perf_counter()\n",
    "diff1 = end-start \n",
    "print(diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.004715699997178\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "######## Sorting of indexes ########\n",
    "\n",
    "body_index = OrderedDict(sorted(body_index.items()))\n",
    "title_index = OrderedDict(sorted(title_index.items()))\n",
    "category_index = OrderedDict(sorted(category_index.items()))\n",
    "infobox_index = OrderedDict(sorted(infobox_index.items()))\n",
    "reference_index = OrderedDict(sorted(reference_index.items()))\n",
    "reference_index = OrderedDict(sorted(reference_index.items()))\n",
    "external_index = OrderedDict(sorted(external_index.items()))\n",
    "end = time.perf_counter()\n",
    "diff2 = end-start\n",
    "print(diff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.408788200002164\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "index_path = './'\n",
    "\n",
    "word_position = defaultdict(dict)\n",
    "fields_list = {'t':title_index,'b':body_index,'c':category_index,'i':infobox_index,'r':reference_index,'e':external_index}\n",
    "\n",
    "for field in fields_list.keys():\n",
    "    LineNum = 1\n",
    "    file_name = index_path + field + '_1.txt'\n",
    "    file = open(file_name, 'w+')\n",
    "    for word in fields_list[field]:\n",
    "        posting = '|'.join(fields_list[field][word]) + '\\n'\n",
    "        file.write(posting)\n",
    "        word_position[field][word], LineNum = LineNum, LineNum+1\n",
    "    file.close()\n",
    "    \n",
    "end = time.perf_counter()\n",
    "diff3 = end-start\n",
    "print(diff3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39099960000021383\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "word_file = open(index_path + \"word_positions.pickle\", \"wb+\")\n",
    "pickle.dump(word_position, word_file)\n",
    "word_file.close()\n",
    "\n",
    "title_file = open(index_path + \"title_position.pickle\", \"wb+\")\n",
    "pickle.dump((title_list,title_position),title_file)\n",
    "title_file.close()\n",
    "\n",
    "end = time.perf_counter()\n",
    "diff4 = end-start\n",
    "print(diff4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:  188.57711559999734\n"
     ]
    }
   ],
   "source": [
    "print('Total time: ', end-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701121\n"
     ]
    }
   ],
   "source": [
    "print(len(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
