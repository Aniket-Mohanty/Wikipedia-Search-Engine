{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# import linecache\n",
    "import linereader as lr\n",
    "import re\n",
    "from collections import *\n",
    "import time\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import Stemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = './final_index/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer.Stemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overhead time:  108.784628\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "file = open(index_path + 'word_positions.pickle','rb')\n",
    "word_positions             =  pickle.load(file)\n",
    "file.close()\n",
    "file = open(index_path + 'title_position.pickle','rb')\n",
    "title_list  =  pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "title_cache = lr.dopen(index_path + 'title.txt')\n",
    "body_cache = lr.dopen(index_path + 'body.txt')\n",
    "category_cache = lr.dopen(index_path + 'category.txt')\n",
    "infobox_cache = lr.dopen(index_path + 'infobox.txt')\n",
    "reference_cache = lr.dopen(index_path + 'reference.txt')\n",
    "external_cache = lr.dopen(index_path + 'external.txt')\n",
    "\n",
    "cache_list = {'t':title_cache,'b':body_cache,'c':category_cache,'i':infobox_cache,'r':reference_cache,'e':external_cache}\n",
    "\n",
    "end = time.perf_counter()\n",
    "print('Overhead time: ',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getline(file_manager,line_num):\n",
    "    \n",
    "    if(line_num == 1):\n",
    "        return file_manager.getline(1)\n",
    "    elif(line_num > 1):\n",
    "        return file_manager.getlines(line_num-1,line_num)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergePost(post1, post2):\n",
    "\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    \n",
    "    p1_len = len(post1)\n",
    "    p2_len = len(post2)\n",
    " \n",
    "    docs = []\n",
    " \n",
    "    while ((ptr1 < p1_len) and (ptr2 < p2_len)):\n",
    "\n",
    "        if(post2[ptr2][0] < post1[ptr1][0]):   \n",
    "            \n",
    "            while((ptr2 < p2_len) and (post2[ptr2][0] < post1[ptr1][0])): ptr2 += 1\n",
    " \n",
    "        elif(post1[ptr1][0] < post2[ptr2][0]): \n",
    "            \n",
    "            while((ptr1 < p1_len) and (post1[ptr1][0] < post2[ptr2][0])): ptr1 += 1\n",
    " \n",
    "        else:\n",
    "            new_score = (post1[ptr1][1] + post2[ptr2][1])\n",
    "            docs.append((post1[ptr1][0], new_score))\n",
    " \n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionPost(post1, post2):\n",
    "    \n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    \n",
    "    p1_len = len(post1)\n",
    "    p2_len = len(post2)\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    while ((ptr1 < p1_len) and (ptr2 < p2_len)):\n",
    "        \n",
    "        if(post1[ptr1][0] < post2[ptr2][0]):\n",
    "            \n",
    "            docs.append(post1[ptr1])\n",
    "            ptr1 += 1\n",
    "        \n",
    "        elif(post1[ptr1][0] > post2[ptr2][0]):\n",
    "            \n",
    "            docs.append(post2[ptr2])\n",
    "            ptr2 += 1\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            docs.append(post1[ptr1])\n",
    "            ptr1,ptr2 = ptr1+1,ptr2+1\n",
    "            \n",
    "    while (ptr1 < p1_len):\n",
    "        \n",
    "        docs.append(post1[ptr1])\n",
    "        ptr1 += 1\n",
    "    \n",
    "    while (ptr2 < p2_len):\n",
    "        \n",
    "        docs.append(post2[ptr2])\n",
    "        ptr2 += 1\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_weight = {'t':10,'b':4,'c':7,'i':5.5,'r':2.5,'e':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeAll(querio, fields, cache_list = cache_list, word_positions = word_positions, fields_weight = fields_weight):\n",
    "    \n",
    "    fields_list = {'t':'title','b':'body','c':'category','i':'infobox','r':'reference','e':'external'}\n",
    "\n",
    "    result = []\n",
    "    flag = 0\n",
    "\n",
    "    for field,quer in zip(fields,querio):\n",
    "\n",
    "        query_words = quer.strip().split(' ')\n",
    "\n",
    "        for word in query_words:\n",
    "\n",
    "            word = stemmer.stemWord(word.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            if(word not in stop_words):\n",
    "                if(word in word_positions[field]):\n",
    "                    file = cache_list[field]\n",
    "                    docs = getline(file,word_positions[field][word])[:-1]\n",
    "                    docs = docs.split('|')\n",
    "                    docs = [list(map(float,doc.split(':'))) for doc in docs]\n",
    "                    docs = [[doc[0], fields_weight[field]*doc[1]] for doc in docs]\n",
    "\n",
    "                    if(not flag): \n",
    "                        result = docs\n",
    "                        flag = 1\n",
    "\n",
    "                    else: result = mergePost(result,docs)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(querio, fields, cache_list = cache_list, word_positions = word_positions, fields_weight = fields_weight):\n",
    "    \n",
    "    fields_list = {'t':'title','b':'body','c':'category','i':'infobox','r':'reference','e':'external'}\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for field,quer in zip(fields,querio):\n",
    "\n",
    "        query_words = quer.strip().split(' ')\n",
    "        word_result = []\n",
    "        \n",
    "        for word in query_words:\n",
    "\n",
    "            word = stemmer.stemWord(word.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            if(word not in stop_words):\n",
    "                if(word in word_positions[field]):\n",
    "                    file = cache_list[field]\n",
    "                    docs = getline(file,word_positions[field][word])[:-1]\n",
    "                    docs = docs.split('|')\n",
    "                    docs = [list(map(float,doc.split(':'))) for doc in docs]\n",
    "                    docs = [[doc[0], fields_weight[field]*doc[1]] for doc in docs]\n",
    "\n",
    "                    if(not word_result): \n",
    "                        word_result = docs\n",
    "\n",
    "                    else: \n",
    "                        word_result = mergePost(word_result,docs)\n",
    "                        if(not word_result):\n",
    "                            word_result = unionPost(word_result,docs)\n",
    "        \n",
    "        if(not result):\n",
    "            result = word_result\n",
    "        else:\n",
    "            result = unionPost(result, word_result)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doomsday_lev1(fqueries, fields, cache_list = cache_list, word_positions = word_positions, fields_weight = fields_weight):\n",
    "    \n",
    "    fields_list = {'t':'title','b':'body','c':'category','i':'infobox','r':'reference','e':'external'}\n",
    "    \n",
    "    result = []\n",
    "    best_result = []\n",
    "    res_max_len = 0\n",
    "    \n",
    "    for idx,(fquery,field)in enumerate(zip(fqueries,fields)):\n",
    "        \n",
    "        result = mergeAll(fqueries[:idx] + fqueries[idx+1:], fields[:idx] + fields[idx+1:])    \n",
    "        \n",
    "        for f in fields_list:\n",
    "            \n",
    "            if(fields_list[f] == field):\n",
    "                continue\n",
    "            \n",
    "            query_words = fquery.strip().split(' ')\n",
    "\n",
    "            for word in query_words:\n",
    "\n",
    "                word = stemmer.stemWord(word.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                if(word not in stop_words):\n",
    "                    if(word in word_positions[f]):\n",
    "                        file = cache_list[f]\n",
    "                        docs = getline(file,word_positions[f][word])[:-1]\n",
    "                        docs = docs.split('|')\n",
    "                        docs = [list(map(float,doc.split(':'))) for doc in docs]\n",
    "                        docs = [[doc[0], fields_weight[f]*doc[1]] for doc in docs]\n",
    "                        \n",
    "                        if(result): \n",
    "                            result = mergePost(result, docs)\n",
    "                            if(result):\n",
    "                                if(len(result) > res_max_len):\n",
    "                                    best_result = result\n",
    "                                    res_max_len = len(result)\n",
    "                        \n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doomsday_lev2(fqueries, fields, cache_list = cache_list, word_positions = word_positions, fields_weight = fields_weight):\n",
    "    \n",
    "    fields_list = {'t':'title','b':'body','c':'category','i':'infobox','r':'reference','e':'external'}\n",
    "    \n",
    "    result = []\n",
    "    best_result = []\n",
    "    res_max_len = 0\n",
    "    \n",
    "    for idx1,(fquery1,field1) in enumerate(zip(fqueries,fields)):\n",
    "        \n",
    "        for idx2,(fquery2,field2) in enumerate(zip(fqueries,fields)):\n",
    "            \n",
    "            result = mergeAll(list(np.delete(np.array(fqueries),[idx1,idx2])), list(np.delete(np.array(fields),[idx1,idx2])))\n",
    "            \n",
    "            for f in fields_list:\n",
    "            \n",
    "                if(fields_list[f] == field1):\n",
    "                    continue\n",
    "\n",
    "                query_words = fquery1.strip().split(' ')\n",
    "\n",
    "                for word in query_words:\n",
    "\n",
    "                    word = stemmer.stemWord(word.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                    if(word not in stop_words):\n",
    "                        if(word in word_positions[f]):\n",
    "                            file = cache_list[f]\n",
    "                            docs = getline(file,word_positions[f][word])[:-1]\n",
    "                            docs = docs.split('|')\n",
    "                            docs = [list(map(float,doc.split(':'))) for doc in docs]\n",
    "                            docs = [[doc[0], fields_weight[f]*doc[1]] for doc in docs]\n",
    "\n",
    "                            if(result): \n",
    "                                result = mergePost(result, docs)\n",
    "                                if(result and len(result) > res_max_len):\n",
    "                                    best_result = result\n",
    "                                    res_max_len = len(result)\n",
    "\n",
    "            if(result):\n",
    "                for f in fields_list:\n",
    "            \n",
    "                    if(fields_list[f] == field2):\n",
    "                        continue\n",
    "\n",
    "                    query_words = fquery2.strip().split(' ')\n",
    "\n",
    "                    for word in query_words:\n",
    "\n",
    "                        word = stemmer.stemWord(word.lower()).translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                        if(word not in stop_words):\n",
    "                            if(word in word_positions[f]):\n",
    "                                file = cache_list[f]\n",
    "                                docs = getline(file,word_positions[f][word])[:-1]\n",
    "                                docs = docs.split('|')\n",
    "                                docs = [list(map(float,doc.split(':'))) for doc in docs]\n",
    "                                docs = [[doc[0], fields_weight[f]*doc[1]] for doc in docs]\n",
    "\n",
    "                                if(result): \n",
    "                                    result = mergePost(result, docs)\n",
    "                                    if(result and len(result) > res_max_len):\n",
    "                                        best_result = result\n",
    "                                        res_max_len = len(result)\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(cache_list, queries, k = 5, word_positions = word_positions, title_list = title_list):\n",
    "        \n",
    "    fields_list = {'t':'title','b':'body','c':'category','i':'infobox','r':'reference','e':'external'}\n",
    "    \n",
    "    search_output = []\n",
    "    \n",
    "    for query in queries:\n",
    "        \n",
    "        flag = 0\n",
    "        result = []\n",
    "        \n",
    "        if ':' in query:\n",
    "            querio = re.sub(r'\\w:',r'|',query).strip('|').split('|')\n",
    "            fields = re.findall(r'\\w:',query)\n",
    "            fields = [f.split(':')[0] for f in fields]\n",
    "            \n",
    "            result = mergeAll(querio, fields)\n",
    "            \n",
    "            if(result and len(result) < k):\n",
    "                result = unionPost(result, unionAll(querio, fields))\n",
    "\n",
    "            if(not result):\n",
    "                result = doomsday_lev1(querio, fields)\n",
    "                if(not result):\n",
    "                    result  = doomsday_lev2(querio, fields)\n",
    "            \n",
    "            if(not result):\n",
    "                result = unionAll(querio, fields)\n",
    "                            \n",
    "        else:\n",
    "            query_t = 't:' + query\n",
    "            querio = re.sub(r'\\w:',r'|',query_t).strip('|').split('|')\n",
    "            fields = re.findall(r'\\w:',query_t)\n",
    "            fields = [f.split(':')[0] for f in fields]\n",
    "            \n",
    "            title_result = mergeAll(querio, fields)\n",
    "            if(title_result and len(title_result) < k):\n",
    "                title_result = unionPost(title_result, unionAll(querio, fields))\n",
    "\n",
    "            if(not title_result):\n",
    "                title_result = doomsday_lev1(querio, fields)\n",
    "                if(not title_result):\n",
    "                    title_result  = doomsday_lev2(querio, fields)\n",
    "            \n",
    "            if(not title_result):\n",
    "                title_result = unionAll(querio, fields)\n",
    "\n",
    "            query_b = 'b:' + query\n",
    "            querio = re.sub(r'\\w:',r'|',query_b).strip('|').split('|')\n",
    "            fields = re.findall(r'\\w:',query_b)\n",
    "            fields = [f.split(':')[0] for f in fields]\n",
    "            \n",
    "            body_result = mergeAll(querio, fields)\n",
    "            \n",
    "            if(body_result and len(body_result) < k):\n",
    "                body_result = unionPost(body_result, unionAll(querio, fields))\n",
    "\n",
    "            if(not body_result):\n",
    "                body_result = doomsday_lev1(querio, fields)\n",
    "                if(not body_result):\n",
    "                    body_result  = doomsday_lev2(querio, fields)\n",
    "            \n",
    "            if(not body_result):\n",
    "                body_result = unionAll(querio, fields)\n",
    "            \n",
    "            query_i = 'i:' + query\n",
    "            querio = re.sub(r'\\w:',r'|',query_i).strip('|').split('|')\n",
    "            fields = re.findall(r'\\w:',query_i)\n",
    "            fields = [f.split(':')[0] for f in fields]\n",
    "            \n",
    "            info_result = mergeAll(querio, fields)\n",
    "            \n",
    "            if(info_result and len(info_result) < k):\n",
    "                info_result = unionPost(info_result, unionAll(querio, fields))\n",
    "\n",
    "            if(not info_result):\n",
    "                info_result = doomsday_lev1(querio, fields)\n",
    "                if(not info_result):\n",
    "                    info_result  = doomsday_lev2(querio, fields)\n",
    "            \n",
    "            if(not info_result):\n",
    "                info_result = unionAll(querio, fields)\n",
    "            \n",
    "            \n",
    "#             general_queries = query.split(' ')\n",
    "            \n",
    "#             title_result = []\n",
    "#             body_result = []\n",
    "            \n",
    "#             for gquery in general_queries:\n",
    "                \n",
    "#                 query_words = gquery.split(' ')\n",
    "\n",
    "#                 field = 't'\n",
    "#                 for word in query_words:\n",
    "                    \n",
    "#                     word = stemmer.stemWord(word.lower())\n",
    "                        \n",
    "#                     if(word not in stop_words):\n",
    "                    \n",
    "#                         if(word in word_positions[field]):\n",
    "#                             file = cache_list[field]\n",
    "#                             docs = file.getline(word_positions[field][word])[:-1]\n",
    "#                             docs = docs.split('|')\n",
    "#                             docs = [list(map(float,doc.split(':'))) for doc in docs]\n",
    "\n",
    "#                             if(not title_result):\n",
    "#                                 title_result = docs\n",
    "#                             else:\n",
    "#                                 tmp = mergePost(result,docs)\n",
    "#                                 if(not tmp):\n",
    "#                                     title_result = unionPost(title_result,docs)\n",
    "#                                 else:\n",
    "#                                     title_result = tmp\n",
    "                \n",
    "#                 field = 'b'\n",
    "#                 for word in query_words:\n",
    "                    \n",
    "#                     word = stemmer.stemWord(word.lower())\n",
    "                        \n",
    "#                     if(word not in stop_words):   \n",
    "#                         if(word in word_positions[field]):\n",
    "#                             file = cache_list[field]\n",
    "#                             docs = file.getline(word_positions[field][word])[:-1]\n",
    "#                             docs = docs.split('|')\n",
    "#                             docs = [list(map(float,doc.split(':'))) for doc in docs]\n",
    "\n",
    "#                             if(not body_result):\n",
    "#                                 body_result = docs\n",
    "#                             else:\n",
    "#                                 tmp = mergePost(body_result,docs)\n",
    "#                                 if(not tmp):\n",
    "#                                     body_result = unionPost(body_result,docs)\n",
    "#                                 else:\n",
    "#                                     body_result = tmp\n",
    "#             if(title_result and len(title_result) >= k):\n",
    "#                 result = title_result\n",
    "#             else:\n",
    "#                 tmp = mergePost(title_result,body_result)\n",
    "#                 if(tmp and len(tmp) >= k):\n",
    "#                     result = tmp\n",
    "#                 else:\n",
    "#                     result = unionPost(title_result,body_result)\n",
    "            result = unionPost(title_result,body_result)\n",
    "            result = unionPost(result,info_result)\n",
    "            \n",
    "        if(result):\n",
    "            result = sorted(result, key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
    "            doc_titles = []\n",
    "            for idx,doc in enumerate(result):\n",
    "                doc_titles.append(str(int(doc[0])) + ', ' + title_list[int(doc[0])-1])\n",
    "#                 if(idx < k):\n",
    "#                     print(doc[1])\n",
    "                if(idx == k-1): break\n",
    "            search_output.append(doc_titles)\n",
    "        else:\n",
    "            search_output.append(['No results found'])\n",
    "        \n",
    "    \n",
    "    return search_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_output(query_file,out_file):\n",
    "    with open(query_file,'r') as qf:\n",
    "        with open(out_file,'w') as outf:\n",
    "            for line in qf:\n",
    "                k = line.split(',')[0]\n",
    "                query = line.split(',')[1].strip()\n",
    "                start = time.perf_counter()\n",
    "                results = search(cache_list,[query],int(k))\n",
    "                end = time.perf_counter()\n",
    "                total_time = end-start\n",
    "                avg_time = total_time/int(k)\n",
    "                for res in results[0]:\n",
    "                    outf.write(res + '\\n')\n",
    "                outf.write(str(total_time) + ', ' + str(avg_time))\n",
    "                outf.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = './20171150_queries1.txt'\n",
    "out_file = './queries_op.txt'\n",
    "give_output(query_file,out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_results 50\n",
      "1000690, Whiplash (Marvel Comics)\n",
      "9712111, The Hand (comics)\n",
      "9703711, Uncle Marvel\n",
      "9699823, Monica Rambeau\n",
      "9472065, Leader (comics)\n",
      "988278, Discrimination against superheroes\n",
      "9593940, Guardians of the Galaxy (1969 team)\n",
      "9485104, Vampire (Marvel Comics)\n",
      "995863, Mister Fear\n",
      "9813810, Marvel Fanfare\n",
      "9698874, Mad Thinker\n",
      "9726577, Blizzard (comics)\n",
      "9703694, Lieutenant Marvels\n",
      "9700979, Captain Marvel Adventures\n",
      "9690245, Cosmic Cube\n",
      "9462837, Ricochet (comics)\n",
      "9737148, Watcher (comics)\n",
      "9392855, Two-Gun Kid\n",
      "9760139, Champions (1975 team)\n",
      "9719240, Xavier's Security Enforcers\n",
      "9658844, Copperhead (Marvel Comics)\n",
      "9703780, Hoppy the Marvel Bunny\n",
      "9529574, Loki (comics)\n",
      "9411137, District X\n",
      "1015661, Hercules (Marvel Comics)\n",
      "981483, Nick Fury's Howling Commandos\n",
      "9712374, X-Factor Investigations\n",
      "1000743, Red Raven (Marvel Comics)\n",
      "991657, Squadron Sinister\n",
      "9744924, DC vs. Marvel\n",
      "9817297, Spider-Ham\n",
      "9566141, Thor (Ultimate Marvel character)\n",
      "9744610, JLA/Avengers\n",
      "9602493, The Fall of the Mutants\n",
      "9759322, Puppet Master (Marvel Comics)\n",
      "9703600, Whiz Comics\n",
      "9701393, Tombstone (comics)\n",
      "9564297, Rockslide (comics)\n",
      "9555512, Sabra (comics)\n",
      "9484248, Typhoid Mary (comics)\n",
      "979580, Magician (Marvel Comics)\n",
      "9806772, Wrecking Crew (comics)\n",
      "9703482, Spitfire (comics)\n",
      "9550295, Cletus Kasady\n",
      "9473973, Slingers (Marvel Comics)\n",
      "9812932, Proteus (Marvel Comics)\n",
      "9763251, Darkstar (Marvel Comics)\n",
      "9455615, Not Brand Echh\n",
      "9395807, Selene (comics)\n",
      "987222, RoboCop (comics)\n",
      "7.5010806000009325 , 0.15002161200001865\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = ['50, b:Marc Spector i:Marvel Comics c:1980 comics debuts']\n",
    "for line in queries:\n",
    "    k = line.split(',')[0]\n",
    "    query = line.split(',')[1].strip()\n",
    "    start = time.perf_counter()\n",
    "    results = search(cache_list,[query],int(k))\n",
    "    end = time.perf_counter()\n",
    "    total_time = end-start\n",
    "    avg_time = total_time/int(k)\n",
    "#     print('num_results', len(results[0]))\n",
    "    print('\\n'.join(results[0]))\n",
    "    print(total_time,',',avg_time)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
