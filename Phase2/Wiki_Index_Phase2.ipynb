{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n",
    "from contextlib import ExitStack\n",
    "\n",
    "import xml.etree.cElementTree as et\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "from collections import *\n",
    "from math import *\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import Stemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_dict = defaultdict(int)\n",
    "for word in stop_words:\n",
    "    stop_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_xml = \"../WikiDump_1/WikiDump_1.xml-p1p30303\"\n",
    "index_path = './index_data/'\n",
    "index_full_path = './full_index/'\n",
    "final_index = './final_index/'\n",
    "\n",
    "if(not os.path.isdir(index_path)): os.mkdir(index_path)\n",
    "if(not os.path.isdir(index_full_path)): os.mkdir(index_full_path)\n",
    "if(not os.path.isdir(final_index)): os.mkdir(final_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_xml = 5\n",
    "data_path = '../Phase2_Data/'\n",
    "wiki_xml_list = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_xml_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_external_links(body):\n",
    "    external_links = []\n",
    "    lines = body.split(\"==\")[-1]\n",
    "    lines = lines.split(\"\\n\")\n",
    " \n",
    "    for line in lines:\n",
    "        if re.match(r\"\\*(.*)\", line):\n",
    "            external_links.append(line)\n",
    " \n",
    "    return external_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki(body):\n",
    "    \n",
    "    body = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub('<!--.*?-->',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub('<math([> ].*?)(</math>|/>)',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub(r'\\[\\[([fF]ile:|[iI]mage)[^]]*(\\]\\])',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub(r'{{v?cite(.*?)}}',' ',body,flags = re.DOTALL)\n",
    "\n",
    "    References = re.findall(\"<ref>(.*?)</ref>\", body)\n",
    "    Infobox = re.findall(r\"\\{\\{Infobox (.*?)\\}\\}\", body, flags = re.DOTALL)\n",
    "    Category = re.findall(r\"\\[\\[Category:(.*?)\\]\\]\", body)\n",
    "    External = get_external_links(body)\n",
    "\n",
    "    body = re.sub('<.*?>',' ',body,flags = re.DOTALL)\n",
    "    body = re.sub('{{([^}{]*)}}','',body,flags = re.DOTALL)\n",
    "    body = re.sub('{{([^}]*)}}','',body,flags = re.DOTALL)\n",
    "\n",
    "    return body.lower(), Infobox, Category, References, External"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer.Stemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_position = []\n",
    "title_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10941.3851526\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "PageCount = 0\n",
    "title_num = 0\n",
    "MAX_FILES = 30000\n",
    "file_count = 0\n",
    "\n",
    "for wiki_xml in wiki_xml_list:\n",
    "    \n",
    "    title_index = defaultdict(list)\n",
    "    body_index = defaultdict(list)\n",
    "    category_index = defaultdict(list)\n",
    "    infobox_index = defaultdict(list)\n",
    "    reference_index = defaultdict(list)\n",
    "    external_index = defaultdict(list)\n",
    "    \n",
    "    stem_dict = defaultdict(int)\n",
    "    \n",
    "#     _ = gc.collect()\n",
    "    \n",
    "    wiki_xml = os.path.join(data_path,wiki_xml)\n",
    "    \n",
    "    for idx, (event, elem) in enumerate(et.iterparse(wiki_xml, events=('start', 'end'))):\n",
    "\n",
    "        tag = elem.tag.split('}')[-1]\n",
    "\n",
    "        if event == 'start':\n",
    "            if tag == 'page':\n",
    "                title_dict = defaultdict(int)\n",
    "                infobox_dict = defaultdict(int)\n",
    "                category_dict = defaultdict(int)\n",
    "                reference_dict = defaultdict(int)\n",
    "                external_dict = defaultdict(int)\n",
    "                id = -1\n",
    "                redirect = ''\n",
    "                inrevision = False\n",
    "                ns = 0\n",
    "                body_dict = defaultdict(int)\n",
    "                body_empty = False\n",
    "            elif tag == 'revision':\n",
    "                # Do not pick up on revision id's\n",
    "                inrevision = True\n",
    "\n",
    "        else:\n",
    "            if tag == 'title':\n",
    "                title = elem.text\n",
    "                if(title):\n",
    "                    title_list.append(title)\n",
    "                    title_num += 1\n",
    "            elif tag == 'id' and not inrevision:\n",
    "                id = int(elem.text)\n",
    "            elif tag == 'redirect':\n",
    "                redirect = elem.attrib['title']\n",
    "            elif tag == 'ns':\n",
    "                ns = int(elem.text)\n",
    "            elif tag == 'text':\n",
    "                body = elem.text\n",
    "                if(body == None):\n",
    "                    body_empty = True\n",
    "            elif tag == 'page':\n",
    "                PageCount += 1\n",
    "\n",
    "                if(not body_empty):\n",
    "                    body,Infobox,Category,Reference,External = clean_wiki(body)\n",
    "                \n",
    "#################### Acquiring word counts per document ####################\n",
    "\n",
    "                #### Title ####\n",
    "                title_words = re.split(\"[^a-zA-Z0-9]\",title.lower())\n",
    "                for word in title_words:\n",
    "                    if(word):\n",
    "                        if(not stem_dict[word]):\n",
    "                            stem_dict[word] = stemmer.stemWord(word)\n",
    "                        word = stem_dict[word]\n",
    "                        if(len(word) > 2):\n",
    "                            title_dict[word] += 1\n",
    "\n",
    "                if(not body_empty):\n",
    "                #### Infobox ####\n",
    "                    for info in Infobox:\n",
    "                        info_words = re.split(\"[^a-zA-Z0-9]\",info.lower())\n",
    "                        for word in info_words:\n",
    "                            if(word and not stop_dict[word]):\n",
    "                                if(not stem_dict[word]):\n",
    "                                    stem_dict[word] = stemmer.stemWord(word)\n",
    "                                word = stem_dict[word]\n",
    "                                if(len(word) > 2):\n",
    "                                    infobox_dict[word] += 1\n",
    "\n",
    "                #### Category ####\n",
    "                    for cate in Category:\n",
    "                        cate_words = re.split(\"[^a-zA-Z0-9]\",cate.lower())\n",
    "                        for word in cate_words:\n",
    "                            if(word and not stop_dict[word]):\n",
    "                                if(not stem_dict[word]):\n",
    "                                    stem_dict[word] = stemmer.stemWord(word)\n",
    "                                word = stem_dict[word]\n",
    "                                if(len(word) > 2):\n",
    "                                    category_dict[word] += 1\n",
    "\n",
    "                #### Reference ####\n",
    "                    for ref in Reference:\n",
    "                        ref_words = re.split(\"[^a-zA-Z0-9]\",ref.lower())\n",
    "                        for word in ref_words:\n",
    "                            if(word and not stop_dict[word]):\n",
    "                                word = stemmer.stemWord(word)\n",
    "                                if(len(word) > 2):\n",
    "                                    reference_dict[word] += 1\n",
    "\n",
    "                #### External ####\n",
    "                    for ext in External:\n",
    "                        ext_words = re.split(\"[^a-zA-Z0-9]\",ext.lower())\n",
    "                        for word in ext_words:\n",
    "                            if(word and not stop_dict[word]):\n",
    "                                if(not stem_dict[word]):\n",
    "                                    stem_dict[word] = stemmer.stemWord(word)\n",
    "                                word = stem_dict[word]\n",
    "                                if(len(word) > 2):\n",
    "                                    external_dict[word] += 1\n",
    "\n",
    "\n",
    "                ##### Body #####\n",
    "                    body_words = re.split(\"[^a-zA-Z0-9]\",body)\n",
    "                    for word in body_words:\n",
    "                        if(word and not stop_dict[word]):\n",
    "                            if(not stem_dict[word]):\n",
    "                                stem_dict[word] = stemmer.stemWord(word)\n",
    "                            word = stem_dict[word]\n",
    "                            if(len(word) > 2):\n",
    "                                body_dict[word] += 1\n",
    "\n",
    "####################### Index Creation #######################\n",
    "\n",
    "                for word in title_dict :\n",
    "                    tf = round(1 + log10(title_dict[word]),3)\n",
    "                    title_index[word].append(':'.join((str(PageCount),str(tf))))\n",
    "\n",
    "                if(not body_empty):\n",
    "                \n",
    "                    for word in body_dict :\n",
    "                        tf = round(1 + log10(body_dict[word]),3)\n",
    "                        body_index[word].append(':'.join((str(PageCount),str(tf))))\n",
    "\n",
    "                    for word in category_dict :\n",
    "                        tf = round(1 + log10(category_dict[word]),3)\n",
    "                        category_index[word].append(':'.join((str(PageCount),str(tf))))\n",
    "\n",
    "                    for word in infobox_dict :\n",
    "                        tf = round(1 + log10(infobox_dict[word]),3)\n",
    "                        infobox_index[word].append(':'.join((str(PageCount),str(tf))))\n",
    "\n",
    "                    for word in reference_dict :\n",
    "                        tf = round(1 + log10(reference_dict[word]),3)\n",
    "                        reference_index[word].append(':'.join((str(PageCount),str(tf))))\n",
    "\n",
    "                    for word in external_dict :\n",
    "                        tf = round(1 + log10(external_dict[word]),3)\n",
    "                        external_index[word].append(':'.join((str(PageCount),str(tf))))\n",
    "\n",
    "\n",
    "############## File creation per MAX_FILES pages ##############\n",
    "\n",
    "                if(PageCount % MAX_FILES == 0):\n",
    "\n",
    "                    stem_dict = defaultdict(int)\n",
    "\n",
    "                    body_index = OrderedDict(sorted(body_index.items()))\n",
    "                    title_index = OrderedDict(sorted(title_index.items()))\n",
    "                    category_index = OrderedDict(sorted(category_index.items()))\n",
    "                    infobox_index = OrderedDict(sorted(infobox_index.items()))\n",
    "                    reference_index = OrderedDict(sorted(reference_index.items()))\n",
    "                    external_index = OrderedDict(sorted(external_index.items()))\n",
    "\n",
    "                    file_name = index_path + 't' + '_' + str(file_count) + '.txt'\n",
    "                    file = open(file_name, 'w+')\n",
    "                    for word in title_index:\n",
    "                        posting = '|'.join(title_index[word]) + '\\n'\n",
    "                        posting = word + '>' + posting\n",
    "                        file.write(posting)\n",
    "                    file.close()\n",
    "\n",
    "                    file_name = index_path + 'b' + '_' + str(file_count) + '.txt'\n",
    "                    file = open(file_name, 'w+')\n",
    "                    for word in body_index:\n",
    "                        posting = '|'.join(body_index[word]) + '\\n'\n",
    "                        posting = word + '>' + posting\n",
    "                        file.write(posting)\n",
    "                    file.close()\n",
    "\n",
    "                    file_name = index_path + 'c' + '_' + str(file_count) + '.txt'\n",
    "                    file = open(file_name, 'w+')\n",
    "                    for word in category_index:\n",
    "                        posting = '|'.join(category_index[word]) + '\\n'\n",
    "                        posting = word + '>' + posting\n",
    "                        file.write(posting)\n",
    "                    file.close()\n",
    "\n",
    "                    file_name = index_path + 'i' + '_' + str(file_count) + '.txt'\n",
    "                    file = open(file_name, 'w+')\n",
    "                    for word in infobox_index:\n",
    "                        posting = '|'.join(infobox_index[word]) + '\\n'\n",
    "                        posting = word + '>' + posting\n",
    "                        file.write(posting)\n",
    "                    file.close()\n",
    "\n",
    "                    file_name = index_path + 'r' + '_' + str(file_count) + '.txt'\n",
    "                    file = open(file_name, 'w+')\n",
    "                    for word in reference_index:\n",
    "                        posting = '|'.join(reference_index[word]) + '\\n'\n",
    "                        posting = word + '>' + posting\n",
    "                        file.write(posting)\n",
    "                    file.close()\n",
    "\n",
    "                    file_name = index_path + 'e' + '_' + str(file_count) + '.txt'\n",
    "                    file = open(file_name, 'w+')\n",
    "                    for word in external_index:\n",
    "                        posting = '|'.join(external_index[word]) + '\\n'\n",
    "                        posting = word + '>' + posting\n",
    "                        file.write(posting)\n",
    "                    file.close()\n",
    "\n",
    "                    title_index = defaultdict(list)\n",
    "                    body_index = defaultdict(list)\n",
    "                    category_index = defaultdict(list)\n",
    "                    infobox_index = defaultdict(list)\n",
    "                    reference_index = defaultdict(list)\n",
    "                    external_index = defaultdict(list)\n",
    "\n",
    "#                     _ = gc.collect()\n",
    "\n",
    "                    file_count += 1\n",
    "\n",
    "            elem.clear()\n",
    "    #     if(PageCount == 30):\n",
    "    #         break\n",
    "\n",
    "########### File creation for last batch of pages ###########\n",
    "    \n",
    "    stem_dict.clear()\n",
    "\n",
    "    body_index = OrderedDict(sorted(body_index.items()))\n",
    "    title_index = OrderedDict(sorted(title_index.items()))\n",
    "    category_index = OrderedDict(sorted(category_index.items()))\n",
    "    infobox_index = OrderedDict(sorted(infobox_index.items()))\n",
    "    reference_index = OrderedDict(sorted(reference_index.items()))\n",
    "    external_index = OrderedDict(sorted(external_index.items()))\n",
    "\n",
    "    file_name = index_path + 't' + '_' + str(file_count) + '.txt'\n",
    "    file = open(file_name, 'w+')\n",
    "    for word in title_index:\n",
    "        posting = '|'.join(title_index[word]) + '\\n'\n",
    "        posting = word + '>' + posting\n",
    "        file.write(posting)\n",
    "    file.close()\n",
    "\n",
    "    file_name = index_path + 'b' + '_' + str(file_count) + '.txt'\n",
    "    file = open(file_name, 'w+')\n",
    "    for word in body_index:\n",
    "        posting = '|'.join(body_index[word]) + '\\n'\n",
    "        posting = word + '>' + posting\n",
    "        file.write(posting)\n",
    "    file.close()\n",
    "\n",
    "    file_name = index_path + 'c' + '_' + str(file_count) + '.txt'\n",
    "    file = open(file_name, 'w+')\n",
    "    for word in category_index:\n",
    "        posting = '|'.join(category_index[word]) + '\\n'\n",
    "        posting = word + '>' + posting\n",
    "        file.write(posting)\n",
    "    file.close()\n",
    "\n",
    "    file_name = index_path + 'i' + '_' + str(file_count) + '.txt'\n",
    "    file = open(file_name, 'w+')\n",
    "    for word in infobox_index:\n",
    "        posting = '|'.join(infobox_index[word]) + '\\n'\n",
    "        posting = word + '>' + posting\n",
    "        file.write(posting)\n",
    "    file.close()\n",
    "\n",
    "    file_name = index_path + 'r' + '_' + str(file_count) + '.txt'\n",
    "    file = open(file_name, 'w+')\n",
    "    for word in reference_index:\n",
    "        posting = '|'.join(reference_index[word]) + '\\n'\n",
    "        posting = word + '>' + posting\n",
    "        file.write(posting)\n",
    "    file.close()\n",
    "\n",
    "    file_name = index_path + 'e' + '_' + str(file_count) + '.txt'\n",
    "    file = open(file_name, 'w+')\n",
    "    for word in external_index:\n",
    "        posting = '|'.join(external_index[word]) + '\\n'\n",
    "        posting = word + '>' + posting\n",
    "        file.write(posting)\n",
    "    file.close()\n",
    "\n",
    "#     _ = gc.collect()\n",
    "    \n",
    "    file_count += 1\n",
    "\n",
    "end = time.perf_counter()\n",
    "diff1 = end-start \n",
    "print(diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(title_list) != title_num):\n",
    "    print('Warning: Might need to track title positions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.32888900000034\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "word_position = defaultdict(dict)\n",
    "fields_list = {'t':'title','b':'body','c':'category','i':'infobox','r':'reference','e':'external'}\n",
    "token_count = 0\n",
    "for field in fields_list.keys():\n",
    "    files = []\n",
    "#     _ = gc.collect()\n",
    "    prev_word = ''\n",
    "    LineNum = 1\n",
    "    with ExitStack() as stack:\n",
    "        for idx in range(file_count):\n",
    "            file_name = index_path + field + '_' + str(idx) + '.txt'\n",
    "            files.append(stack.enter_context(open(file_name)))\n",
    "        with open(index_full_path + fields_list[field] + '.txt', 'w') as f:\n",
    "                file_iter = heapq.merge(*files)\n",
    "                for idx,line in enumerate(file_iter):\n",
    "                    line = line.split('>')\n",
    "                    word = line[0]\n",
    "                    posting_list = line[1][:-1]\n",
    "                    if(word == prev_word):\n",
    "                        f.write('|' + posting_list)\n",
    "                    else:\n",
    "                        word_position[field][word], LineNum = LineNum, LineNum+1\n",
    "                        if(idx == 0):\n",
    "#                             f.write(word + '-' + posting_list)\n",
    "                            f.write(posting_list)\n",
    "                        else:\n",
    "#                             f.write('\\n' + word + '-' + posting_list)\n",
    "                            f.write('\\n' + posting_list)\n",
    "                        \n",
    "                        if(field == 'b'): token_count += 1\n",
    "                \n",
    "                    prev_word = word\n",
    "end = time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  11848676\n"
     ]
    }
   ],
   "source": [
    "print('Number of tokens: ',token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3309.2646637000003\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "fields_list = {'t':'title','b':'body','c':'category','i':'infobox','r':'reference','e':'external'}\n",
    "PageCount = 9829059\n",
    "term_weights = {'title':1,'body':1,'category':1,'infobox':1,'reference':1,'external':1}\n",
    "for field in fields_list.values():\n",
    "    with open(index_full_path + field + '.txt','r') as fr:\n",
    "        with open(final_index + field + '.txt','w+') as fw:\n",
    "            for line in fr:\n",
    "                docs = line.split('|')\n",
    "                docs = [doc.split(':') for doc in docs]\n",
    "                docs = dict(sorted(docs,key = lambda kv:(int(kv[0]), kv[1])))\n",
    "\n",
    "                values = np.array(list(map(float,docs.values())))\n",
    "                num_docs_with_word = len(docs)\n",
    "                idf = log10(PageCount/num_docs_with_word)\n",
    "                values = np.round(term_weights[field]*values*idf,2)\n",
    "\n",
    "                new_line = []\n",
    "#                 _ = gc.collect()\n",
    "                \n",
    "                for doc,val in zip(docs.keys(),values):\n",
    "                    new_line.append(doc + ':' + str(val))\n",
    "\n",
    "                new_line = '|'.join(new_line)\n",
    "\n",
    "                fw.write(new_line + '\\n')\n",
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.113533100000495\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "word_file = open(final_index + \"word_positions.pickle\", \"wb+\")\n",
    "pickle.dump(word_position, word_file)\n",
    "word_file.close()\n",
    "\n",
    "title_file = open(final_index + \"title_position.pickle\", \"wb+\")\n",
    "pickle.dump(title_list,title_file)\n",
    "title_file.close()\n",
    "\n",
    "end = time.perf_counter()\n",
    "diff4 = end-start\n",
    "print(diff4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total time: ', end-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
